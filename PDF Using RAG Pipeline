import os
import PyPDF2
import openai
from sentence_transformers import SentenceTransformer
from pinecone import Index
import faiss
PDF_DIRECTORY = "path/to/pdf_directory"
VECTOR_DB = "path/to/vector_database"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
CHUNK_SIZE = 512
embedding_model = SentenceTransformer(EMBEDDING_MODEL)
vector_dimension = embedding_model.get_sentence_embedding_dimension()
faiss_index = faiss.IndexFlatL2(vector_dimension)

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file."""
    text = ""
    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
    return text

def chunk_text(text, chunk_size):
    """Chunks the text into smaller pieces."""
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

def embed_chunks(chunks):
    """Generates embeddings for text chunks."""
    return embedding_model.encode(chunks, convert_to_tensor=False)

def store_embeddings_in_faiss(embeddings):
    """Stores embeddings in FAISS index."""
    faiss_index.add(embeddings)

def ingest_pdfs(directory):
    """Ingests PDFs from a directory, chunks, and stores embeddings."""
    metadata = []
    for file_name in os.listdir(directory):
        if file_name.endswith('.pdf'):
            file_path = os.path.join(directory, file_name)
            text = extract_text_from_pdf(file_path)
            chunks = chunk_text(text, CHUNK_SIZE)
            embeddings = embed_chunks(chunks)
            store_embeddings_in_faiss(embeddings)
            metadata.extend([(file_name, chunk) for chunk in chunks])
    return metadata

def query_faiss_index(query, k=5):
    """Queries FAISS index for top-k relevant chunks."""
    query_embedding = embedding_model.encode([query])[0]
    distances, indices = faiss_index.search(query_embedding.reshape(1, -1), k)
    return indices

def generate_response(query, relevant_chunks):
    """Generates response using OpenAI's LLM API."""
    prompt = f"Based on the following information: {relevant_chunks}, answer: {query}"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

if __name__ == "__main__":
    metadata = ingest_pdfs(PDF_DIRECTORY)
    user_query = "What are the key points in the annual report?"
    relevant_indices = query_faiss_index(user_query)
    relevant_chunks = [metadata[i] for i in relevant_indices[0]]
    response = generate_response(user_query, relevant_chunks)
    print("Response:", response)


# The code initializes a pipeline to process PDF files, extract text, chunk it into smaller pieces, store embeddings in a FAISS vector database, and retrieve relevant chunks to generate responses using an LLM.
